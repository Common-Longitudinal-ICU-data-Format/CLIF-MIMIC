{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init [RUN THIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import polars as pl\n",
    "import os, sys\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# print(os.getcwd())\n",
    "os.chdir('/gpfs/data/healthcare-allocate/CLIF-MIMIC/code')\n",
    "# print(os.getcwd())\n",
    "\n",
    "proj_root = \"/gpfs/data/healthcare-allocate/CLIF-MIMIC\"\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.append(proj_root)\n",
    "\n",
    "# from code.custom_utils import *\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mimic_table(module: {\"icu\", \"hosp\"}, table, file_type: {\"csv\", \"parquet\", \"pq\"} = \"csv\"):\n",
    "    if file_type in [\"pq\", \"parquet\"]:\n",
    "        return pd.read_parquet(f'../mimic-iv-2.2/{module}/{table}.parquet')\n",
    "    elif file_type == \"csv\":\n",
    "        return pd.read_csv(f'../mimic-iv-2.2/{module}/{table}.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = load_mimic_table(\"hosp\", \"patients\") # gives gender\n",
    "admissions = load_mimic_table(\"hosp\", \"admissions\") # gives race and ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items = load_mimic_table(\"icu\", \"d_items\", \"csv\")\n",
    "chartevents = load_mimic_table(\"icu\", \"chartevents\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedureevents = load_mimic_table(\"icu\", \"procedureevents\", \"csv\")\n",
    "datetimeevents = load_mimic_table(\"icu\", \"datetimeevents\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputevents = load_mimic_table(\"icu\", \"inputevents\", \"csv\")\n",
    "outputevents = load_mimic_table(\"icu\", \"outputevents\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resave_mimic_table_to_parquet(table: pd.DataFrame):\n",
    "    # if not yet in memory, load it:\n",
    "    # if not table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labevents = load_mimic_table(\"hosp\", \"labevents\", \"csv\")\n",
    "# labevents.to_parquet(\"../mimic-iv-2.2/hosp/labevents.parquet\")\n",
    "d_labitems = load_mimic_table(\"hosp\", \"d_labitems\", \"csv\")\n",
    "labevents = load_mimic_table(\"hosp\", \"labevents\", \"parquet\")\n",
    "\n",
    "poe = load_mimic_table(\"hosp\", \"poe\", \"csv\")\n",
    "poe_detail = load_mimic_table(\"hosp\", \"poe_detail\", \"csv\")\n",
    "\n",
    "transfers = load_mimic_table(\"hosp\", \"transfers\", \"csv\")\n",
    "icustays = load_mimic_table(\"icu\", \"icustays\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_events = load_mimic_table(\"icu\", \"ingredientevents\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_csv(csv_name: str, dtype = None):\n",
    "    return pd.read_csv(\n",
    "        f\"../mapping/mimic-to-clif-mappings - {csv_name}.csv\", dtype = dtype\n",
    "        )\n",
    "# covert to a dict for df col renaming later\n",
    "\n",
    "def construct_mapper_dict(\n",
    "    mapping_df: pd.DataFrame, key_col: str, value_col: str, map_none_to_none = False,\n",
    "    excluded_item_ids: list = None\n",
    "    ):\n",
    "\n",
    "    if not excluded_item_ids:\n",
    "        excluded_item_ids = []\n",
    "    \n",
    "    if \"itemid\" in mapping_df.columns:\n",
    "        mapping_df = mapping_df.loc[\n",
    "            ~mapping_df[\"itemid\"].isin(excluded_item_ids)\n",
    "            , \n",
    "            ]\n",
    "    \n",
    "    mapper_dict = dict(zip(mapping_df[key_col], mapping_df[value_col]))\n",
    "    \n",
    "    # Replace \"NO MAPPING\" with NA\n",
    "    for key, value in mapper_dict.items():\n",
    "        if value == \"NO MAPPING\":\n",
    "            mapper_dict[key] = None\n",
    "    \n",
    "    # to enable a None -> None mapping\n",
    "    if map_none_to_none:\n",
    "        mapper_dict[None] = None\n",
    "        \n",
    "    return mapper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient table\n",
    "race_ethnicity_mapping = load_mapping_csv(\"race_ethnicity\")\n",
    "race_mapper_dict = construct_mapper_dict(race_ethnicity_mapping, \"mimic_race\", \"race\")\n",
    "ethnicity_mapper_dict = construct_mapper_dict(race_ethnicity_mapping, \"mimic_race\", \"ethnicity\")\n",
    "\n",
    "# hosp table\n",
    "discharge_mapping = load_mapping_csv(\"discharge\")\n",
    "discharge_mapper_dict = construct_mapper_dict(\n",
    "    discharge_mapping, \"discharge_location\", \"disposition_category\"\n",
    "    )\n",
    "\n",
    "# adt \n",
    "adt_mapping = load_mapping_csv(\"adt\")\n",
    "adt_mapper_dict = construct_mapper_dict(adt_mapping, \"careunit\", \"location_category\")\n",
    "\n",
    "# vitals table\n",
    "vitals_mapping = load_mapping_csv(\"vitals\")\n",
    "vital_name_mapper_dict = construct_mapper_dict(vitals_mapping, \"itemid\", \"label = vital_name\")\n",
    "vital_category_mapper_dict = construct_mapper_dict(vitals_mapping, \"itemid\", \"vital_category\")\n",
    "\n",
    "# resp support table\n",
    "resp_mapping = load_mapping_csv(\"respiratory_support\")\n",
    "resp_device_mapping = load_mapping_csv(\"device_category\")\n",
    "resp_mode_mapping = load_mapping_csv(\"mode_category\")\n",
    "\n",
    "resp_mapper_dict = construct_mapper_dict(resp_mapping, \"itemid\", \"variable\")\n",
    "resp_device_mapper_dict = construct_mapper_dict(\n",
    "    resp_device_mapping, \"device_name\", \"device_category\", excluded_item_ids = [\"223848\"]\n",
    "    )\n",
    "resp_mode_mapper_dict = construct_mapper_dict(resp_mode_mapping, \"mode_name\", \"mode_category\")\n",
    "\n",
    "# labs\n",
    "labs_mapping = load_mapping_csv(\"labs\")\n",
    "labs_mapping[\"itemid\"] = labs_mapping[\"itemid\"].dropna().astype(int).astype(str)\n",
    "labs_mapper_dict = construct_mapper_dict(labs_mapping, \"itemid\", \"lab_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_mapper_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils [RUN THIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level = logging.INFO,\n",
    "    format = '%(asctime)s:%(levelname)s:%(message)s',\n",
    "    handlers = [logging.FileHandler(\"mimic-to-clif.log\"),\n",
    "              logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CacheInfo` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheInfo:\n",
    "    \"\"\"\n",
    "    CacheInfo object used to represent the current status of `lru_cache`\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.misses = 0\n",
    "        self.hits = 0\n",
    "        self.cur_size = 0\n",
    "        # NOTE: you may add to this if you want, but do not modify the lines above\n",
    "        # create an attribute in the CacheInfo class to store the cache dict\n",
    "        self.cache_dict = {}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CacheInfo(hits={self.hits}, misses={self.misses}, max_size={self.max_size}, cur_size={self.cur_size})\"\n",
    "\n",
    "# first layer: a decorator factory\n",
    "def lru_cache(max_size = 128):\n",
    "    '''\n",
    "    This function is a decorator factory that returns a decorator with a user-specified\n",
    "    maximum size of the cache \n",
    "\n",
    "    Input:\n",
    "        - max_size: the maximum size of the cache\n",
    "    \n",
    "    Output: \n",
    "        - a decorator\n",
    "    '''\n",
    "    # second layer: the decorator\n",
    "    def decorator(func):\n",
    "        '''\n",
    "        This function is a decorator that takes in an original function and\n",
    "        return a new, decorated function.\n",
    "\n",
    "        Input: an original function\n",
    "\n",
    "        Output: a new function\n",
    "        '''\n",
    "        # initialize an instance of the CacheInfo class\n",
    "        cache_info = CacheInfo(max_size = max_size)\n",
    "        def key_generator(*args, **kwargs):\n",
    "            '''\n",
    "            This helper function creates a unique key given every different \n",
    "            combination of positional and key-word arguments\n",
    "\n",
    "            Input: \n",
    "                - *args: any position arguments\n",
    "                - **kwargs: any key word arguments\n",
    "\n",
    "            Output: \n",
    "                - a tuple that stores all the arguments and their data type\n",
    "            '''\n",
    "            # generates a tuple that stores the data type of each position arg\n",
    "            args_type = tuple(map(lambda x: type(x), args))\n",
    "            # kwargs is a dict, so we use items() to turn it into a seq of\n",
    "            # key-value tuples, and add the data type of the key word arg\n",
    "            # to the tuple, then use frozenset() to make it immutable and \n",
    "            # thus hashable\n",
    "            kwargs_and_type = frozenset(\n",
    "                       map(lambda tup: (tup, type(tup[1])), \n",
    "                           kwargs.items()))\n",
    "            return (args, args_type, kwargs_and_type)\n",
    "        def new_func(*args, **kwargs):\n",
    "            '''\n",
    "            This is the new function that replaces the original function.\n",
    "\n",
    "            Input:\n",
    "                - *args, **kwargs: any position and key word arguments\n",
    "\n",
    "            Output:\n",
    "                - the result of the new function\n",
    "            '''\n",
    "            key = key_generator(*args, **kwargs)        \n",
    "            # if the key is already in the cache dict, i.e. the same args have\n",
    "            # been provided before, there should be \"memory\" in the cache\n",
    "            # we got a hit\n",
    "            if key in cache_info.cache_dict:\n",
    "                cache_info.hits += 1 \n",
    "                # temporarily store the cache result first before we remove the key\n",
    "                cached_result = cache_info.cache_dict[key]\n",
    "                # remove the key\n",
    "                cache_info.cache_dict.pop(key)\n",
    "                # insert the same key to the tail of the dict\n",
    "                cache_info.cache_dict[key] = cached_result\n",
    "                return cached_result\n",
    "            # when we have a new arg combination that is not seen before \n",
    "            # -- we have a \"miss\":\n",
    "            else:\n",
    "                cache_info.misses += 1  \n",
    "                # add the output of the function to the dict\n",
    "                cache_info.cache_dict[key] = func(*args, **kwargs)  \n",
    "                # update cache size (length of the dict) \n",
    "                cache_info.cur_size = len(cache_info.cache_dict) \n",
    "                # if the cache exceeds the maximum size, remove the least recently used item\n",
    "                if len(cache_info.cache_dict) > max_size:\n",
    "                    # first covert the dict to a list so we can track the order\n",
    "                    cache_list = list(cache_info.cache_dict.items())\n",
    "                    # remove the first element in the list, which is the least \n",
    "                    # recently used item\n",
    "                    cache_list.pop(0)\n",
    "                    # convert the list back to a dict and update\n",
    "                    cache_info.cache_dict = dict(cache_list)\n",
    "                    # update the cache size again, which should = max_size\n",
    "                    cache_info.cur_size = len(cache_info.cache_dict)  \n",
    "                return cache_info.cache_dict[key]\n",
    "        # update the attribute of the now-decorated new func\n",
    "        new_func.cache_info = cache_info\n",
    "        return new_func     \n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_sort_datetime(df: pd.DataFrame, event_type: {\"ce\", \"pe\", \"other\"}, time_col_name = \"time\"):\n",
    "    # if procedure event\n",
    "    if event_type == \"pe\":\n",
    "        df[\"starttime\"] = pd.to_datetime(df[\"starttime\"])\n",
    "        df[\"endtime\"] = pd.to_datetime(df[\"endtime\"])\n",
    "        df = df.sort_values([\"hadm_id\", \"starttime\", \"endtime\", \"storetime\"]).reset_index(drop = True) #.reset_index()\n",
    "    elif event_type == \"ce\":\n",
    "        df[\"charttime\"] = pd.to_datetime(df['charttime'])\n",
    "        df = df.sort_values([\"hadm_id\", \"charttime\", \"storetime\"]).reset_index(drop = True) #.reset_index()\n",
    "    elif event_type == \"other\":\n",
    "        df[\"time\"] = pd.to_datetime(df['time'])\n",
    "        df = df.sort_values([\"hadm_id\", \"time\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_rclif(df: pd.DataFrame, table_name, file_format = \"pq\"):\n",
    "    if file_format in [\"pq\", \"parquet\"]:\n",
    "        df.to_parquet(f'../rclif/clif_{table_name}.parquet')\n",
    "\n",
    "def read_from_rclif(table_name, file_format = \"pq\"):\n",
    "    if file_format in [\"pq\", \"parquet\"]:\n",
    "        return pd.read_parquet(f'../rclif/clif_{table_name}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: delete \"ALREDAY MAPPED\" at some pt\n",
    "EXCLUDED_LABELS_DEFAULT = [\"NO MAPPING\", \"UNSURE\", \"MAPPED ELSEWHERE\", \"SPECIAL CASE\", \"ALREADY MAPPED\"] \n",
    "\n",
    "# find all the relevant item ids for a table\n",
    "def get_relevant_item_ids(mapping_df: pd.DataFrame, decision_col: str, \n",
    "                          excluded_labels: list = EXCLUDED_LABELS_DEFAULT,\n",
    "                          excluded_item_ids: list = None\n",
    "                          ):\n",
    "    '''\n",
    "    - decision_col: the col on which to apply the excluded_labels\n",
    "    - excluded_item_ids: additional item ids to exclude\n",
    "    '''\n",
    "    if not excluded_item_ids:\n",
    "        excluded_item_ids = []\n",
    "    \n",
    "    return mapping_df.loc[\n",
    "        (~mapping_df[decision_col].isin(excluded_labels)) & \n",
    "        (~mapping_df[\"itemid\"].isin(excluded_item_ids))\n",
    "        , \"itemid\"\n",
    "        ].unique()\n",
    "    \n",
    "def rename_and_reorder_cols(df, rename_mapper_dict: dict, new_col_order: list) -> pd.DataFrame:\n",
    "    baseline_rename_mapper = {\n",
    "        \"subject_id\": \"patient_id\", \"hadm_id\": \"hospitalization_id\",\n",
    "    }\n",
    "    \n",
    "    return (\n",
    "        df.rename(columns = baseline_rename_mapper | rename_mapper_dict)\n",
    "        .reindex(columns = new_col_order)\n",
    "        )\n",
    "\n",
    "def find_duplicates(df, cols: list[str] = [\"hadm_id\", \"time\", \"itemid\"]):\n",
    "    '''\n",
    "    Check whether there are duplicates -- more than one populated value -- for what is supposed to be \n",
    "    unique combination of columns. That is, for the same measured variable (e.g. vital_category) at\n",
    "    the same time during the same encounter, there should be only one corresponding value.\n",
    "    '''\n",
    "    return df[df.duplicated(subset = cols, keep = False)]\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame, additional_cols: list):\n",
    "    '''\n",
    "    Check whether there are duplicates -- more than one populated value -- for what is supposed to be \n",
    "    unique combination of columns. That is, for the same measured variable (e.g. vital_category) at\n",
    "    the same time during the same hospitalization, there should be only one corresponding value.\n",
    "    '''\n",
    "    cols_to_check = [\"hospitalization_id\", \"recorded_dttm\"].extend(additional_cols)\n",
    "    return df[df.duplicated(subset = cols_to_check, keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def item_id_to_feature_value(item_id: int, col: str = \"label\", df = d_items):\n",
    "    '''\n",
    "    Find the corresponding feature value of an item by id.\n",
    "    i.e. find the label, or linksto, of item id 226732.\n",
    "    '''\n",
    "    row = df.loc[df[\"itemid\"] == item_id, :]\n",
    "    label = row[\"label\"].values[0]\n",
    "    if col == \"label\":\n",
    "        logging.info(f\"the {col} for item {item_id} is {label}\")\n",
    "        return label\n",
    "    else:\n",
    "        feature_value = row[col].values[0]\n",
    "        logging.info(f\"the {col} for item {item_id} ({label}) is {feature_value}\")\n",
    "        return feature_value\n",
    "\n",
    "@lru_cache()\n",
    "def item_id_to_label(item_id: int) -> str:\n",
    "    '''\n",
    "    Helper function that returns the \"label\" string of an item given its item_id. \n",
    "    '''\n",
    "    return item_id_to_feature_value(item_id)\n",
    "\n",
    "def item_id_to_events_df(item_id: int, original: bool = False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return in a pandas df all the events associated with an item id.\n",
    "    - simplify: whether to return the original df (False), or a simplified one \n",
    "    with some columns (particulary timestamps) renamed to support integration \n",
    "    between different events df.  # FIXME - might rename this arg\n",
    "    '''\n",
    "    # find whether it is chartevents, or procedure events, etc.\n",
    "    linksto_table_name = item_id_to_feature_value(item_id, col = \"linksto\")\n",
    "    # turn string into a dj object\n",
    "    linksto_df: pd.DataFrame = globals()[linksto_table_name]\n",
    "    events_df = linksto_df.loc[linksto_df[\"itemid\"] == item_id, :]\n",
    "    # return the original columns\n",
    "    if original:\n",
    "        return events_df\n",
    "    # else, if simplified:\n",
    "    elif linksto_table_name == \"procedureevents\": # FIXME: trach is complex and need additional attention\n",
    "        events_df_simplified = events_df.loc[\n",
    "            :, ['subject_id', 'hadm_id', 'stay_id', 'endtime', 'itemid', 'value', 'valueuom']\n",
    "        ].rename(columns = {\"endtime\": \"time\"})\n",
    "        return events_df_simplified\n",
    "    elif linksto_table_name == \"chartevents\":\n",
    "        events_df_simplified = events_df.loc[\n",
    "            :, ['subject_id', 'hadm_id', 'stay_id', 'charttime', 'itemid', 'value', 'valueuom']\n",
    "        ].rename(columns = {\"charttime\": \"time\"})\n",
    "        return events_df_simplified\n",
    "        \n",
    "    # FIXME: likely an issue if data struct of different events table are different \n",
    "\n",
    "def item_ids_list_to_events_df(item_ids: list):\n",
    "    df_list = [item_id_to_events_df(item_id, original = True) for item_id in item_ids]\n",
    "    df_merged = pd.concat(df_list) #.head().assign(\n",
    "        ## linksto = lambda df: df[\"itemid\"].apply(lambda item_id: item_id_to_feature_value(item_id, col = \"linksto\"))\n",
    "    # )\n",
    "    return df_merged \n",
    "    # FIXME: automatically add the label and linksto table source columns -- create cache?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ItemFinder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemFinder():\n",
    "    def __init__(self, kw = None, items_df = d_items, \n",
    "                 col: str = \"label\", case_sensitive: bool = False, \n",
    "                 for_labs: bool = False, report_na = True\n",
    "                 ) -> pd.DataFrame:\n",
    "        '''\n",
    "        Look up an item by keyword from the `d_items` table of the `icu` module.\n",
    "        - case: whether the search is case sensitive\n",
    "        - report_na: whether to print when there is no match; or simply return a \n",
    "        '''\n",
    "        self.kw = kw \n",
    "        self.df = items_df\n",
    "        self.col = \"abbreviation\" if col == \"abbr\" else col\n",
    "        self.for_labs = for_labs\n",
    "\n",
    "        # df of items that match the key words -- a raw output\n",
    "        self.items_select_df: pd.DataFrame = items_df[\n",
    "            items_df[self.col].str.contains(kw, case = case_sensitive, na = False)\n",
    "        ]\n",
    "        \n",
    "        # first check whether there is any return in the raw output\n",
    "        if len(self.items_select_df) == 0:\n",
    "            if report_na:\n",
    "                raise Exception(f\"No matching result found in column {col} with case sensitive being {case_sensitive}\")\n",
    "            else:\n",
    "                logging.warning(f\"No matching result for {kw} in column {col} with case sensitive being {case_sensitive}\")\n",
    "                self.candidate_table = pd.DataFrame()\n",
    "        \n",
    "        # ... only proceed when the return is not of zero length\n",
    "        # and enhance the simple raw output with counts and value instances\n",
    "        else:\n",
    "            logging.info(f\"{len(self.items_select_df)} matching item(s) found for {self.kw}.\")\n",
    "            # list of ids for items that match the key words\n",
    "            self.items_select_ids = self.items_select_df[\"itemid\"].values\n",
    "            # a np array of non-duplicated events table names, e.g. [\"chartevents\", \"procedureevents\"]\n",
    "            self.linksto_table_names = self.items_select_df[\"linksto\"].unique()\n",
    "            self.item_freq = self.generate_item_freq()\n",
    "            # logging.info(f\"type is {type(self.item_freq)}\")\n",
    "            self.candidate_table = self.make_candidate_table()\n",
    "\n",
    "    def generate_item_freq(self):\n",
    "        '''\n",
    "        Iterative over each events table, find the items freq therein, and combine into one df.\n",
    "        # FIXME - should maybe make this map style without loop\n",
    "        '''\n",
    "        freq_df_ls = [] # a list of df's\n",
    "        for table_name in self.linksto_table_names:\n",
    "            # fetch the object by name str, i.e. chartevents, procedureevents, etc.\n",
    "            events_df: pd.DataFrame = globals()[table_name]\n",
    "            # filter for all the selected events in that events table\n",
    "            events_select_df = events_df.loc[\n",
    "                events_df[\"itemid\"].isin(self.items_select_ids), :\n",
    "            ]\n",
    "            # a df of item freq for one event type  \n",
    "            item_freq = events_select_df.value_counts(\"itemid\")\n",
    "            item_freq.name = \"count\"\n",
    "\n",
    "            # check if the df is empty -- there shouldn't be an empty one FIXME\n",
    "            # if not item_freq_df.empty:\n",
    "            freq_df_ls.append(item_freq)\n",
    "        \n",
    "        return pd.concat(freq_df_ls)\n",
    "\n",
    "    def make_candidate_table(self):\n",
    "        '''\n",
    "        merge item freq and values instances to the raw output to generate the enhanced table of the \n",
    "        candidate items.\n",
    "        '''\n",
    "        \n",
    "        cand_table = (\n",
    "            self.items_select_df\n",
    "            .loc[:, [\"itemid\", \"label\", \"abbreviation\", \"linksto\", \"category\", \"unitname\", \"param_type\"]]\n",
    "            # FIXME\n",
    "            .join(self.item_freq, on = \"itemid\", validate = \"1:1\")\n",
    "            .sort_values(by = \"count\", ascending = False) \n",
    "            .assign(\n",
    "                value_instances = lambda df: df[\"itemid\"].apply(item_id_to_value_instances)\n",
    "            )\n",
    "        )\n",
    "        if self.for_labs:\n",
    "            return cand_table.reindex(\n",
    "                columns = [\"itemid\", \"label\", \"abbreviation\", \"linksto\", \"category\", \"count\", \"value_instances\", \"unitname\"]\n",
    "                )\n",
    "        else:\n",
    "            return cand_table\n",
    "\n",
    "@lru_cache()\n",
    "def item_id_to_value_instances(item_id: int):\n",
    "    '''\n",
    "    Wrapper\n",
    "    '''\n",
    "    label = item_id_to_feature_value(item_id, \"label\")\n",
    "\n",
    "    param_type = item_id_to_feature_value(item_id, \"param_type\")\n",
    "    \n",
    "    if param_type == \"Numeric\":\n",
    "        val_instances = item_id_to_value_instances_numeric(item_id)\n",
    "    elif param_type == \"Text\":\n",
    "        val_instances = item_id_to_value_instances_categorical(item_id).to_dict()\n",
    "    else:\n",
    "        return param_type\n",
    "    print(f\"item label: {label}; value instances: {str(val_instances)}\")\n",
    "    return str(val_instances)\n",
    "\n",
    "def item_id_to_value_instances_categorical(item_id: int, events: pd.DataFrame = chartevents):\n",
    "    '''\n",
    "    Return all the unique categories\n",
    "    '''\n",
    "    assoc_events = events.loc[events[\"itemid\"] == item_id, :]\n",
    "    categories: pd.Series = assoc_events.value_counts(\"value\") \n",
    "    return categories\n",
    "    \n",
    "def item_id_to_value_instances_numeric(item_id: int, events: pd.DataFrame = chartevents):\n",
    "    '''\n",
    "    Find max, min, mean of a continuous, or numeric, item.\n",
    "    '''\n",
    "    valuenum_col = events.loc[events[\"itemid\"] == item_id, :][\"valuenum\"]\n",
    "    val_max, val_min, val_mean = valuenum_col.max(), valuenum_col.min(), round(valuenum_col.mean(), 2)\n",
    "    return f\"Max: {val_max}, Min: {val_min}, Mean: {val_mean}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemFinder(\"weight\").candidate_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `patient` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_col_names = [\n",
    "    \"patient_id\", \"race_name\", \"race_category\", \"ethnicity_name\", \"ethnicity_category\",\n",
    "    \"sex_name\", \"sex_category\", \"birth_date\", \"death_dttm\", \"language_name\", \"language_category\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patient_admitted = admissions[\"subject_id\"].nunique()\n",
    "n_patient_admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple race for one patient\n",
    "race_counts = admissions.groupby('subject_id')['race'].nunique()\n",
    "multi_race_indices = race_counts[race_counts > 1].index\n",
    "multi_race_encounters = admissions[\n",
    "    admissions['subject_id'].isin(multi_race_indices)\n",
    "    ][[\"subject_id\", \"hadm_id\", \"race\", \"admittime\", \"admission_type\", \"admission_location\"]]\n",
    "multi_race_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of patients with multiple races over different encounters\n",
    "multi_race_encounters[\"subject_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but only one race per encounter: \n",
    "race_counts = admissions.groupby('hadm_id')['race'].nunique()\n",
    "race_counts[race_counts > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for South Americans\n",
    "south_american_subject_ids = admissions.loc[admissions[\"race\"] == \"SOUTH AMERICAN\", \"subject_id\"].unique()\n",
    "sa_race_counts = (\n",
    "    admissions[admissions[\"subject_id\"].isin(south_american_subject_ids)]\n",
    "    .groupby('subject_id')['race'].nunique()\n",
    ")\n",
    "multi_race_indices_sa = sa_race_counts[sa_race_counts > 1].index\n",
    "multi_race_encounters_sa = admissions[\n",
    "    admissions['subject_id'].isin(multi_race_indices_sa)\n",
    "    ][[\"subject_id\", \"hadm_id\", \"race\", \"admittime\", \"admission_type\", \"admission_location\"]]\n",
    "multi_race_encounters_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sa_race_counts == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_race_encounters[\"subject_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking language issue\n",
    "admissions.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender / sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch sex (intended in CLIF) / gender (available in MIMIC)\n",
    "sex = patients[[\"subject_id\", \"gender\"]].copy()\n",
    "sex.columns = [\"patient_id\", \"sex_name\"]\n",
    "sex[\"sex_category\"] = sex[\"sex_name\"].map(lambda x: \"Female\" if x == \"F\" else \"Male\")\n",
    "sex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NA\n",
    "patients[\"gender\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### race and ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race and ethnicity\n",
    "race_ethn = admissions[[\"subject_id\", \"hadm_id\", \"race\", \"admittime\"]].copy()\n",
    "race_ethn.columns = [\"patient_id\", \"hospitalization_id\", \"race_name\", \"admittime\"]\n",
    "race_ethn[\"race_category\"] = race_ethn[\"race_name\"].map(race_mapper_dict)\n",
    "race_ethn[\"ethnicity_name\"] = race_ethn[\"race_name\"]\n",
    "race_ethn[\"ethnicity_category\"] = race_ethn[\"ethnicity_name\"].map(ethnicity_mapper_dict)\n",
    "race_ethn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multi_race_over_encounters(df, col: str = \"race_category\"):\n",
    "    race_counts = df.groupby('patient_id')[col].nunique()\n",
    "    multi_race_indices = race_counts[race_counts > 1].index\n",
    "    multi_race_encounters = df[\n",
    "        df['patient_id'].isin(multi_race_indices)\n",
    "        ]\n",
    "    return multi_race_encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_multi_race_over_encounters(race_ethn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethn_informative = race_ethn.loc[~race_ethn[\"race_category\"].isin([\"Other\", \"Unknown\"]), ]\n",
    "multi_race_ethn_informative = check_multi_race_over_encounters(race_ethn_informative)\n",
    "multi_race_ethn_informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of patients with multi informative races over diff encounters:\n",
    "multi_race_ethn_informative[\"patient_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for ethnicity - no issue here\n",
    "check_multi_race_over_encounters(multi_race_ethn_informative, col = \"ethnicity_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESUME ETL:\n",
    "# apply de-deduplication logic to create one-to-one mapping from patient_id to race\n",
    "multi_race_deduped = (\n",
    "    multi_race_ethn_informative.groupby('patient_id')\n",
    "    .apply(lambda x: (\n",
    "        x.groupby('race_category')\n",
    "        .agg(count=('race_category', 'size'),\n",
    "             most_recent=('admittime', 'max'))\n",
    "        .sort_values(['count', 'most_recent'], ascending=[False, False])\n",
    "        .head(1)))\n",
    "    .reset_index()\n",
    "    )\n",
    "multi_race_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_race_mapper_dict = dict(zip(multi_race_deduped[\"patient_id\"], multi_race_deduped[\"race_category\"]))\n",
    "\n",
    "race_ethn_deduped = race_ethn.drop_duplicates([\"patient_id\", \"race_category\", \"ethnicity_category\"]).copy()\n",
    "\n",
    "race_ethn_deduped[\"race_category\"] = np.where(\n",
    "    race_ethn_deduped[\"patient_id\"].isin(multi_race_deduped[\"patient_id\"]),\n",
    "    race_ethn_deduped[\"patient_id\"].map(unique_race_mapper_dict),\n",
    "    race_ethn_deduped[\"race_category\"]\n",
    ")\n",
    "\n",
    "race_ethn_deduped.drop_duplicates([\"patient_id\", \"race_category\", \"ethnicity_category\"], inplace=True)\n",
    "\n",
    "race_ethn_deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethn_deduped.value_counts(\"race_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the non-informative others unless they are the only race\n",
    "race_ethn_deduped_informative = race_ethn_deduped.groupby(\"patient_id\").apply(\n",
    "    lambda gr: gr if len(gr) == 1 else gr[~gr[\"race_category\"].isin([\"Other\",\"Unknown\"])]\n",
    ").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same for ethnicity\n",
    "race_ethn_deduped_informative = race_ethn_deduped.groupby(\"patient_id\").apply(\n",
    "    lambda gr: gr if len(gr) == 1 else gr[~gr[\"ethnicity_category\"].isin([\"Other\", \"Unknown\", \"Non-Hispanic\"])]\n",
    ").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates and confirm there is none\n",
    "race_ethn_deduped_informative[\n",
    "    race_ethn_deduped_informative.duplicated([\"patient_id\", \"race_category\"], keep=False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again for ethn\n",
    "race_ethn_deduped_informative[\n",
    "    race_ethn_deduped_informative.duplicated([\"patient_id\", \"ethnicity_category\"], keep=False)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethn_deduped_informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "death = admissions[[\"subject_id\", \"deathtime\"]].copy().dropna(subset=[\"deathtime\"]).drop_duplicates()\n",
    "death.columns = [\"patient_id\", \"death_dttm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_merged = pd.merge(\n",
    "    race_ethn_deduped_informative, sex, on = \"patient_id\", how = \"outer\"\n",
    ")\n",
    "\n",
    "patient_merged = pd.merge(\n",
    "    patient_merged, death, on = \"patient_id\", how = \"outer\", indicator = True)\n",
    "\n",
    "patient_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_merged.value_counts(\"_merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_final = patient_merged.reindex(columns = patient_col_names)\n",
    "patient_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_final[\"hospitalization_id\"] = patient_final[\"hospitalization_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "patient_final.to_parquet('../rclif/clif_patient.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `hospitalization` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_col_names = [\n",
    "    \"patient_id\", \"hospitalization_id\", \"admission_dttm\", \"discharge_dttm\",\n",
    "    \"age_at_admission\", \"admission_type_name\", \"admission_type_category\",\n",
    "    \"discharge_name\", \"discharge_category\", \"zipcode_nine_digit\", \"zipcode_five_digit\", \n",
    "    \"census_block_code\", \"census_block_group_code\", \"census_tract\", \"state_code\", \"county_code\"\n",
    "]\n",
    "\n",
    "hosp_col_rename_mapper = {\n",
    "    \"admittime\": \"admission_dttm\", \"dischtime\": \"discharge_dttm\", \n",
    "    \"admission_type\": \"admission_type_name\", \"discharge_location\": \"discharge_name\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the discharge locations\n",
    "admissions.value_counts(\"discharge_location\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp = admissions[\n",
    "    [\"subject_id\", \"hadm_id\", \"admittime\", \"dischtime\", \"admission_type\", \"discharge_location\"]\n",
    "    ]\n",
    "\n",
    "hosp[\"discharge_category\"] = hosp[\"discharge_location\"].map(discharge_mapper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_merged = pd.merge(\n",
    "    hosp, patients[[\"subject_id\", \"anchor_age\", \"anchor_year\"]],\n",
    "    on = \"subject_id\", how = \"left\"\n",
    ")\n",
    "\n",
    "hosp_merged[\"age_at_admission\"] = hosp_merged[\"anchor_age\"] + pd.to_datetime(hosp_merged[\"admittime\"]).dt.year - hosp_merged[\"anchor_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_final = rename_and_reorder_cols(hosp_merged, hosp_col_rename_mapper, hosp_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_final.to_parquet('../rclif/clif_hospitalization.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ADT` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_col_names = [\"hospitalization_id\", \"hospital_id\", \"in_dttm\", \"out_dttm\", \"location_name\", \"location_category\"]\n",
    "\n",
    "adt_col_rename_mapper = {\n",
    "    'intime': 'in_dttm', 'outtime': 'out_dttm', 'careunit': 'location_name'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemFinder(\"Appendectomy\").candidate_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_to_label(225966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers.value_counts(\"careunit\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers.value_counts(\"eventtype\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_events_units = transfers.value_counts([\"eventtype\", \"careunit\"], dropna=False).reset_index()\n",
    "adt_events_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop two NA cases: (1) discharge -- so careunit is NA; \n",
    "# (2) ED visit with no hospitalization -- so no hadm_id\n",
    "adt = transfers.dropna(subset = [\"careunit\", \"hadm_id\"])\n",
    "adt['location_category'] = adt['careunit'].map(adt_mapper_dict)\n",
    "adt_final = rename_and_reorder_cols(adt, adt_col_rename_mapper, adt_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `vitals` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_mapping = load_mapping_csv(\"vitals\")\n",
    "vital_name_mapper_dict = construct_mapper_dict(vitals_mapping, \"itemid\", \"label = vital_name\")\n",
    "vital_category_mapper_dict = construct_mapper_dict(vitals_mapping, \"itemid\", \"vital_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current logic is temp_site is preserved into meas_site_name; else is NA\n",
    "vital_col_names = [\"hospitalization_id\", \"recorded_dttm\", \"vital_name\", \"vital_category\", \"vital_value\", \"meas_site_name\"]\n",
    "\n",
    "vitals_col_rename_mapper_dict = {\n",
    "    \"hadm_id\": \"hospitalization_id\", \n",
    "    \"time\": \"recorded_dttm\",\n",
    "    \"value\": \"vital_value\"\n",
    "    }\n",
    "\n",
    "@lru_cache()\n",
    "def convert_f_to_c(temp_f) -> float:\n",
    "    if isinstance(temp_f, str) or isinstance(temp_f, int):\n",
    "        temp_f = float(temp_f) \n",
    "    \n",
    "    if isinstance(temp_f, float):\n",
    "        temp_c = (temp_f - 32) * 5 / 9\n",
    "        return round(temp_c, 1) # so 39.3333 -> 39.3\n",
    "    else:\n",
    "        raise(\"wrong type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular cases\n",
    "We first process the regular cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find vital_items_ids\n",
    "vitals_items_ids = get_relevant_item_ids(\n",
    "    mapping_df = vitals_mapping, decision_col = \"vital_category\", \n",
    "    excluded_labels = EXCLUDED_LABELS_DEFAULT + [\"temp_c\"]\n",
    "    )\n",
    "vitals_events = item_ids_list_to_events_df(vitals_items_ids)\n",
    "\n",
    "# use np.where to convert the unit for one item \n",
    "# from lb to kg for the only weight item in undesired unit -- Admission Weight (lbs.)\n",
    "vitals_events[\"value\"] = np.where(\n",
    "    vitals_events[\"itemid\"] == 226531,\n",
    "    vitals_events[\"value\"].astype(float).apply(lambda x: round(x/2.205, 1)),\n",
    "    vitals_events[\"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 35s to run - the same if we are using apply vs map\n",
    "vitals_events[\"vital_name\"] = vitals_events[\"itemid\"].apply(lambda x: vital_name_mapper_dict[x])\n",
    "vitals_events[\"vital_category\"] = vitals_events[\"itemid\"].apply(lambda x: vital_category_mapper_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_final = rename_and_reorder_cols(vitals_events, vitals_col_rename_mapper_dict, vital_col_names)\n",
    "vitals_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked and there is no dup\n",
    "check_duplicates(vitals_final, [\"vital_category\", \"vital_value\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_final[vitals_final[\"vital_value\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special case: `temp_c` \n",
    "We then process the special case where we not only need to convert units but the conversion may also create duplication that needs to be resolved (e.g. if the same measurement was originally recorded in two units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_events = item_ids_list_to_events_df([223761, 223762, 224642])\n",
    "temp_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot directly\n",
    "temp_wider = temp_events.pivot(\n",
    "    index = [\"hadm_id\", \"time\"], \n",
    "    columns = \"itemid\",\n",
    "    values = \"value\"\n",
    "    ).reset_index()\n",
    "temp_wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map temp_site to the clif categories of meas_site_name\n",
    "temp_wider[\"meas_site_name\"] = temp_wider[224642]\n",
    "\n",
    "# convert temp from f to c with a coalesce logic\n",
    "# 223761 = temp in f, 223762 = temp in c\n",
    "temp_wider[\"vital_value\"] = temp_wider[223762].fillna(\n",
    "    temp_wider[223761].apply(convert_f_to_c)\n",
    "    )\n",
    "\n",
    "temp_wider['vital_name'] = temp_wider.apply(\n",
    "    lambda row: \"Temperature Celsius\" if pd.notna(row[223762]) else \"Temperature Fahrenheit\", \n",
    "    axis = \"columns\"\n",
    "    )\n",
    "\n",
    "temp_wider[\"vital_category\"] = \"temp_c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = rename_and_reorder_cols(temp_wider, vitals_col_rename_mapper_dict, vital_col_names)\n",
    "temp_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked there is no dup\n",
    "check_duplicates(temp_final, [\"vital_category\",\t\"vital_value\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NAs and notice that a lot of NAs were generated during pivoting\n",
    "temp_final[temp_final[\"vital_value\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so drop these NAs\n",
    "temp_final.dropna(subset=[\"vital_value\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge, validate, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge \n",
    "vitals_merged = pd.concat([\n",
    "    vitals_final, temp_final\n",
    "])\n",
    "\n",
    "vitals_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there is no more NA\n",
    "vitals_merged[vitals_merged[\"vital_value\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dytpes\n",
    "vitals_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtypes to the desired\n",
    "vitals_merged[\"vital_value\"] = vitals_merged[\"vital_value\"].apply(float)\n",
    "vitals_merged[\"recorded_dttm\"] = pd.to_datetime(vitals_merged[\"recorded_dttm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "vitals_merged.to_parquet('../rclif/clif_vitals.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `respiratory_support` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils [RUN THIS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_item_ids = get_relevant_item_ids(\n",
    "    mapping_df = resp_mapping, decision_col = \"variable\" # , excluded_item_ids=[223848] # remove the vent brand name\n",
    "    ) \n",
    "\n",
    "resp_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_events: pd.DataFrame = item_ids_list_to_events_df(resp_item_ids)\n",
    "resp_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_events[\"variable\"] = resp_events[\"itemid\"].apply(lambda x: resp_mapper_dict[x])\n",
    "resp_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_columns = [\n",
    "    \"hospitalization_id\", \"recorded_dttm\", \"device_name\", \"device_category\", \"vent_brand_name\", \n",
    "    \"mode_name\", \"mode_category\", \"tracheostomy\", \"fio2_set\", \"lpm_set\",\n",
    "    \"tidal_volume_set\", \"resp_rate_set\", \"pressure_control_set\", \"pressure_support_set\",\n",
    "    \"flow_rate_set\", \"peak_inspiratory_pressure_set\", \"inspiratory_time_set\",\n",
    "    \"peep_set\", \"tidal_volume_obs\", \"resp_rate_obs\", \"plateau_pressure_obs\",\n",
    "    \"peak_inspiratory_pressure_obs\", \"peep_obs\", \"minute_vent_obs\", \"mean_airway_pressure_obs\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_device_rank = [\"IMV\", \"NIPPV\", \"CPAP\", \"High Flow NC\", \"Face Mask\", \"Trach Collar\", \"Nasal Cannula\", \"Room Air\", \"Other\"]\n",
    "# testing\n",
    "resp_device_rank.index(\"IMV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_duplicates.query(\"itemid == 224696\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2166-07-19 23:50:00\t2166-07-20 01:20:08\n",
    "resp_events.query(\"stay_id == 39214730\").sort_values(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2166-07-20 04:34:07 -> 2166-07-21 18:06:26\n",
    "resp_events.query(\"stay_id == 36123037\").sort_values(\"time\").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icustays.query(\"hadm_id == 26871621\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the time ranges for the two stays, and see which stay should the 1 am measurement fall into\n",
    "# so should be stay_id == 39214730\n",
    "transfers.query(\"hadm_id == 26871621\").sort_values(\"intime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL [RUN THIS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates to prepare for pivoting \n",
    "# two kinds of duplicates to handle: by devices and other\n",
    "resp_duplicates: pd.DataFrame = find_duplicates(resp_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_duplicates.value_counts(\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_device_mapper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/ deal with devices\n",
    "resp_duplicates_devices: pd.DataFrame = resp_duplicates.query(\"itemid == 226732\").copy()\n",
    "resp_duplicates_devices[\"device_category\"] = resp_duplicates_devices[\"value\"].apply(\n",
    "    lambda x: resp_device_mapper_dict[x.strip()] if pd.notna(x) else None\n",
    "    )\n",
    "resp_duplicates_devices.dropna(subset=\"device_category\",inplace=True)\n",
    "resp_duplicates_devices[\"rank\"] = resp_duplicates_devices[\"device_category\"].apply(\n",
    "    lambda x: resp_device_rank.index(x.strip()))\n",
    "resp_duplicates_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_duplicates_devices#.value_counts(\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the device case - find indices to drop\n",
    "top_ranked_device_indices = resp_duplicates_devices.groupby([\"hadm_id\", \"time\", \"itemid\"])[\"rank\"].idxmin()\n",
    "# non top-ranked categories to be dropped\n",
    "lower_ranked_device_indices = resp_duplicates_devices.index.difference(top_ranked_device_indices)\n",
    "# drop the designated indices\n",
    "resp_events_clean = resp_events.drop(lower_ranked_device_indices)\n",
    "# drop None\n",
    "resp_events_clean.dropna(subset = \"value\", inplace=True) # RESUME\n",
    "resp_events_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_events_clean.value_counts(\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/ deal with duplicate vent reads:\n",
    "setting_duplicate_indices_to_drop = find_duplicates(resp_events_clean).query(\"stay_id == 36123037\").index\n",
    "resp_events_clean.drop(setting_duplicate_indices_to_drop, inplace = True)\n",
    "resp_events_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all duplicates are dropped\n",
    "find_duplicates(resp_events_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two columns based on item_id: \n",
    "resp_events_clean[\"label\"] = resp_events_clean[\"itemid\"].map(item_id_to_label)\n",
    "resp_events_clean[\"variable\"] = resp_events_clean[\"itemid\"].map(resp_mapper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_events_clean.value_counts('itemid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivoting and coalescing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for EDA\n",
    "resp_wider_in_lables = resp_events_clean.pivot(\n",
    "    index = [\"hadm_id\", \"time\"], \n",
    "    columns = [\"variable\", \"label\"],\n",
    "    values = \"value\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_wider_in_lables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for actually cleaning based on item ids\n",
    "resp_wider_in_ids = resp_events_clean.pivot(\n",
    "    index = [\"hadm_id\", \"time\"], \n",
    "    columns = [\"itemid\"],\n",
    "    values = \"value\" \n",
    ").reset_index()\n",
    "resp_wider_in_ids = convert_and_sort_datetime(resp_wider_in_ids, event_type = \"other\")\n",
    "resp_wider_in_ids.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the coalease logic\n",
    "resp_wider_in_ids[\"tracheostomy\"] = resp_wider_in_ids[225448].fillna(resp_wider_in_ids[226237])\n",
    "resp_wider_in_ids[\"lpm_set\"] = resp_wider_in_ids[223834].fillna(resp_wider_in_ids[227287])\n",
    "resp_wider_in_ids[\"tidal_volume_obs\"] = (\n",
    "    resp_wider_in_ids[224685].fillna(resp_wider_in_ids[224686]).fillna(resp_wider_in_ids[224421])\n",
    "    )\n",
    "resp_wider_in_ids[\"resp_rate_set\"] = resp_wider_in_ids[224688].fillna(resp_wider_in_ids[227581])\n",
    "resp_wider_in_ids[\"resp_rate_obs\"] = resp_wider_in_ids[224690].fillna(resp_wider_in_ids[224422])\n",
    "resp_wider_in_ids[\"flow_rate_set\"] = resp_wider_in_ids[224691].fillna(resp_wider_in_ids[227582])\n",
    "resp_wider_in_ids[\"peep_set\"] = resp_wider_in_ids[220339].fillna(resp_wider_in_ids[227579])\n",
    "resp_wider_in_ids[\"mode_name\"] = (\n",
    "    resp_wider_in_ids[223849].fillna(resp_wider_in_ids[229314].fillna(resp_wider_in_ids[227577])) # FIXME\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate variable columns that were coaleased into one\n",
    "resp_wider_cleaned = resp_wider_in_ids.drop(\n",
    "    columns = [225448, 226237, 223834, 227287, 224685, 224686, 224421, 224688, 227581, 224690, \n",
    "               224422, 224691, 227582, 220339, 227579, 223849, 229314, 227577]\n",
    "    )\n",
    "resp_wider_cleaned.rename(columns=resp_mapper_dict, inplace = True)\n",
    "resp_wider_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether there are still duplicates that remain\n",
    "(resp_wider_cleaned.columns.value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_wider_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map _name to _category\n",
    "resp_wider_cleaned[\"device_category\"] = resp_wider_cleaned[\"device_name\"].map(\n",
    "    lambda x: resp_device_mapper_dict[x.strip()] if pd.notna(x) else None\n",
    "    )\n",
    "resp_wider_cleaned[\"mode_category\"] = resp_wider_cleaned[\"mode_name\"].map(resp_mode_mapper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for mapping -- looks like it's fine\n",
    "resp_wider_cleaned.value_counts([\"device_name\", \"device_category\"], dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for mapping\n",
    "resp_counts = resp_wider_cleaned.value_counts([\"mode_name\", \"mode_category\", \"device_category\"], dropna = False).reset_index()\n",
    "resp_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for mapping\n",
    "mode_device_counts = resp_wider_cleaned.value_counts([\"mode_category\", \"device_category\"], dropna = False)\n",
    "mode_device_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_final = rename_and_reorder_cols(\n",
    "    resp_wider_cleaned, \n",
    "    rename_mapper_dict = {\"hadm_id\": \"hospitalization_id\", \"time\": \"recorded_dttm\"}, \n",
    "    new_col_order = resp_columns\n",
    ")\n",
    "resp_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_rclif(resp_final, \"respiratory_support\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
